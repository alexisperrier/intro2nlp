{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9768c5ba",
   "metadata": {},
   "source": [
    "# Vectorize Text for Exploration Using Word Embeddings\n",
    "\n",
    "[![Open In Colab](colab-badge.svg)](https://colab.research.google.com/github/alexisperrier/intro2nlp/blob/master/notebooks/intro2nlp_08_word_embeddings.ipynb)\n",
    "\n",
    "Word embeddings are numerical representations of words or phrases that capture the meaning of the words in a  vector space. They are useful for natural language processing tasks because they capture the semantic relationships between words, which allows algorithms to make more accurate predictions and decisions based on the meaning of the text.\n",
    "\n",
    "This notebook follows chapter 3 of the [intro to NLP](https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing) course on [openclassrooms](https://openclassrooms.com).\n",
    "\n",
    "We will use the gensim library version 4. Note that Gensim has had a significant [upgrade from version 3 to 4](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
    "\n",
    "\n",
    "## Measuring the similarity between words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef875f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install gensim with\n",
    "!pip install --upgrade gensim\n",
    "# check that the version is 4+\n",
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7485a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other libs we will need\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81901899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim and load the model\n",
    "# this might take awhile, especially on google colab.\n",
    "# you can use a lighter smaller model and follow along. \n",
    "# The results will be slightly different but the conclusions will roughly be the same\n",
    "\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# or if the above model takes too long to download, use\n",
    "# model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f14bba",
   "metadata": {},
   "source": [
    "the vector for the work 'book' has dimension 300. It is obtained with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['book'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfefe03",
   "metadata": {},
   "source": [
    "The  `most_similar()`  function returns the 10 most similar words and their similarity scores:\n",
    "python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312a642",
   "metadata": {},
   "source": [
    "Similarly, the words most similar to apple are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93833d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e95ae",
   "metadata": {},
   "source": [
    "whereas, Apple with a capital is associated with the brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"Apple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d372136",
   "metadata": {},
   "source": [
    "We can also measure the similarity score between pair of words. For instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e70600",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"similarity score between apple and banana:\", model.similarity(\"apple\", \"banana\")) \n",
    "print(\"similarity score between apple and dog:   \", model.similarity(\"apple\", \"dog\")) \n",
    "print(\"similarity score between cat   and dog:   \", model.similarity(\"cat\", \"dog\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272e3a6",
   "metadata": {},
   "source": [
    "According to word2vec, a cat is more similar to a dog than an apple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc213a",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "Let's take a look at the vocabulary in the word2vec model.\n",
    "\n",
    "Note: this syntax has changed between Gensim 3.x and 4.x. \n",
    "\n",
    "In 3.x, you would get the vocab with\n",
    "\n",
    "```\n",
    "vocab = model.vocab.keys()\n",
    "```\n",
    "In 4.x, you need\n",
    "\n",
    "```\n",
    "vocab = model.index_to_key\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 5 random words out of the whole available vocabulary, do it 10 times\n",
    "vocab = model.index_to_key\n",
    "for _ in range(10):\n",
    "    print(np.random.choice(vocab,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171fafb",
   "metadata": {},
   "source": [
    "#### side note: levenshtein distance and cosine similarity\n",
    "\n",
    "Cosine similarity can be calculated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "vector1 = [1, 1, 2, 2, 3]\n",
    "vector2 = [1, 3, 1, 2, 6]\n",
    "\n",
    "cosine_similarity = 1 - spatial.distance.cosine(vector1, vector2)\n",
    "print (cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d5678",
   "metadata": {},
   "source": [
    "The cosine similarity for 2 words such as grass and tree is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"similarity score between grass and tree:\", model.similarity(\"grass\", \"tree\")) \n",
    "\n",
    "# and with scipy\n",
    "cosine_similarity = 1 - spatial.distance.cosine(model['grass'], model['tree'])\n",
    "print (\"cosine similarity between grass and tree:\",cosine_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295cd53b",
   "metadata": {},
   "source": [
    "Similarity between words can be measured with other methods. \n",
    "The levenshtein distance measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"distance('test','test') = {distance('test','test')}  because no character substitution is needed\")\n",
    "print(f\"distance('test','team') = {distance('test','team')}  because two character substitutions are needed: s -> a and t -> m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d058e",
   "metadata": {},
   "source": [
    "##### Cultural biais\n",
    "\n",
    "In the US, Alexis is a feminin name, while in the rest of the world it's a masculin name. Word2vec was trained on US centric data. This shows up when looking at the names the model condsiders most similar to 'Alexis': Nicole, Erica, Marissa, Alicia ... all women names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('Alexis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9400b2",
   "metadata": {},
   "source": [
    "##### Out of Vocabulary OOV\n",
    "\n",
    "Some words are not in Word2vec vocab's. for instance Covid and ... word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b31db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.index_to_key\n",
    "\n",
    "# no covid words (only 'covidien' which is a company)\n",
    "start_with = 'covid'\n",
    "vocab_subset = [tk.lower() for tk in  vocab if tk.lower()[:len(start_with)] == start_with]\n",
    "vocab_subset.sort()\n",
    "print(vocab_subset)\n",
    "\n",
    "# no word2vec words\n",
    "start_with = 'word2vec'\n",
    "vocab_subset = [tk.lower() for tk in  vocab if tk.lower()[:len(start_with)] == start_with]\n",
    "vocab_subset.sort()\n",
    "print(vocab_subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f92db",
   "metadata": {},
   "source": [
    "## Train Your First Embedding Models\n",
    "\n",
    "To train your first model, weâ€™ll use the Shakespeare corpus, composed of all the lines of all the Shakespeare plays available on Kaggle (or here). The idea behind working on classic literature is not to be snobbish, but to find a corpus different enough from the ones word2vec and GloVe were trained on (Google U.S. News and Wikipedia). We expect the Shakespeare dataset to have a different worldview and vocabulary. The dataset is also large and already in a short-sequence format, which will speed up the calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96641604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/alexisperrier/intro2nlp/master/data/Shakespeare_alllines.txt'\n",
    "\n",
    "r = requests.get(url)\n",
    "lines = r.text.encode('ascii',errors='ignore').decode('utf-8').split(\"\\n\")\n",
    "\n",
    "# remove all punctuation and only keep verses with more than one token to reduce the size of the corpus\n",
    "sentences = []\n",
    "\n",
    "for line in lines:\n",
    "   # remove punctuation\n",
    "   line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
    "\n",
    "   # simple tokenizer\n",
    "   tokens = re.findall(r'\\b\\w+\\b', line)\n",
    "\n",
    "   # only keep lines with at least one token\n",
    "   if len(tokens) > 1:\n",
    "      sentences.append(tokens)\n",
    "print(\"This gives: \", len(sentences), \"sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c55099",
   "metadata": {},
   "source": [
    "Let's train a word2vec model, which we will call bard2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "bard2vec = Word2Vec(\n",
    "         sentences,\n",
    "         min_count=3,   # Ignore words that appear less than this\n",
    "         vector_size=50,       # Dimensionality of word embeddings\n",
    "         sg = 1,        # skipgrams\n",
    "         window=7,      # Context window for words during training\n",
    "         epochs=40)       # Number of epochs training over corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d81a65",
   "metadata": {},
   "source": [
    "Once the training is done, we can explore the results by looking at word similarity for certain words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e0c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(word):\n",
    "    print(\"-- most similar words to: \", word)\n",
    "    for (token, score) in bard2vec.wv.most_similar(word):\n",
    "        print(f\"\\t{token:>10} {np.round(score,2)}\")\n",
    "    print()\n",
    "    \n",
    "similar_words('King')\n",
    "similar_words('sword')\n",
    "similar_words('husband')\n",
    "similar_words('Hamlet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f39302",
   "metadata": {},
   "source": [
    "The results are dependent on how we trained the model. \n",
    "Let's compare with a model that is trained for a longer time and for larger window\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535196f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "bard2vec = Word2Vec(\n",
    "         sentences,\n",
    "         min_count=3,   # same\n",
    "         vector_size=50,  # same\n",
    "         sg = 0,        # cbow instead of skip-grams\n",
    "         window=10,      # larger context windows\n",
    "         epochs=100)       # longer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(word):\n",
    "    print(\"-- most similar words to: \", word)\n",
    "    for (token, score) in bard2vec.wv.most_similar(word):\n",
    "        print(f\"\\t{token:>10} {np.round(score,2)}\")\n",
    "    print()\n",
    "    \n",
    "similar_words('King')\n",
    "similar_words('sword')\n",
    "similar_words('husband')\n",
    "similar_words('Hamlet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a68d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
