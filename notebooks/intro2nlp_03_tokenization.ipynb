{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "[![Open In Colab](colab-badge.svg)](https://colab.research.google.com/github/alexisperrier/intro2nlp/blob/master/notebooks/intro2nlp_03_tokenization.ipynb)\n",
    "\n",
    "\n",
    "Tokenization or [Text segmentation](https://en.wikipedia.org/wiki/Text_segmentation) is the problem of dividing a string of written language into its component words. \n",
    "\n",
    "The most simple way to divide a text into a list of its words is to split over the whitespaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Let's eat, grandpa\"\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with that approach is that contractions (Let's -> Let + s) are not handled and punctuations signs stay attached to the nearest word (eat, -> eat + ,)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right way to tokenize is to use a tokenizer. Most NLP libraries offer their own tokenizers. Here we will use tokenizers from the [NLTK](https://www.nltk.org/) library.\n",
    "\n",
    "The NLTK library offers many [tokenizers](https://www.nltk.org/api/nltk.tokenize.html). We'll work with the WordPunctTokenizer.\n",
    "\n",
    "But first let's install NLTK and download the necessary resources.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the WordPunctTokenizer\n",
    "\n",
    "We get a different results. The punctuations are now handled as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokens = WordPunctTokenizer().tokenize(\"Let's eat your soup, Grandpa.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the text from the Wikipedia Earth page and look at the frequency of the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "def wikipedia_page(title):\n",
    "    '''\n",
    "    This function returns the raw text of a wikipedia page \n",
    "    given a wikipedia page title\n",
    "    '''\n",
    "    params = { \n",
    "        'action': 'query', \n",
    "        'format': 'json', # request json formatted content\n",
    "        'titles': title, # title of the wikipedia page\n",
    "        'prop': 'extracts', \n",
    "        'explaintext': True\n",
    "    }\n",
    "    # send a request to the wikipedia api \n",
    "    response = requests.get(\n",
    "         'https://en.wikipedia.org/w/api.php',\n",
    "         params= params\n",
    "     ).json()\n",
    "\n",
    "    # Parse the result\n",
    "    page = next(iter(response['query']['pages'].values()))\n",
    "    # return the page content \n",
    "    if 'extract' in page.keys():\n",
    "        return page['extract']\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wikipedia_page('Earth').lower()\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "print(Counter(tokens).most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that earth and earth's for instance are no longer separate tokens and that the punctuation signs are stand alone tokens. This will come in handy if we want to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization on characters\n",
    "\n",
    "We can also tokenize on characters instead of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of character tokenization\n",
    "char_tokens = [ c for c in text ]\n",
    "print(\"Most common characters in the text\")\n",
    "print(Counter(char_tokens).most_common(20))\n",
    "print()\n",
    "print(f\"All characters in the text: \\n{set(char_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "\n",
    "Some words are better taken together: New York, Happy end, Wall street, Linear regression etc ... . When tokenizing we want to consider all possible adjacent pairs of words in the text. We can do this with the NLTK ngrams function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "text = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\".lower()\n",
    "\n",
    "# Tokenize\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "# bigrams \n",
    "bigrams = [w for w in  ngrams(tokens,n=2)]\n",
    "print(bigrams)\n",
    "\n",
    "print()\n",
    "bigrams = ['_'.join(bg) for bg in bigrams]\n",
    "print(bigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for trigrams\n",
    "\n",
    "trigrams = ['_'.join(w) for w in  ngrams(tokens,n=3)]\n",
    "\n",
    "print(trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add ngrams to list of tokens\n",
    "Let's add the bigrams and trigrams to the list of tokens on the wikipedia Earth page and look at the frequency of ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wikipedia_page('Earth').lower()\n",
    "unigrams = WordPunctTokenizer().tokenize(text)\n",
    "bigrams = ['_'.join(w) for w in  ngrams(unigrams,n=2)]\n",
    "trigrams = ['_'.join(w) for w in  ngrams(unigrams,n=3)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = unigrams + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"we have a total of {len(tokens)} tokens, including: \\n- {len(unigrams)} unigrams \\n- {len(bigrams)} bigrams \\n- {len(trigrams)} trigrams. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have multiple bigrams in the top 50 tokens:\n",
    "\n",
    "- of_the\n",
    "- of_earth\n",
    "- in_the\n",
    "\n",
    "Adding ngrams to a list of tokens may help down the line when classifying text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
