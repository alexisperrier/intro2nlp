{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply a Simple Bag-of-Words Approach\n",
    "\n",
    "[![Open In Colab](colab-badge.svg)](https://colab.research.google.com/github/alexisperrier/intro2nlp/blob/master/notebooks/intro2nlp_06_07_vectorizers.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy if you haven't done so already and download the small English model\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# install scikit learn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    '2 cups of flour',\n",
    "    'replace the flour',\n",
    "    'replace the keyboard in 2 minutes',\n",
    "    'do you prefer Windows or Mac',\n",
    "    'the Mac has the most noisy keyboard',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The simplified brown corpus is available in the data folder of this github repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/brown_corpus_extract_humor_science_fiction.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spacy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that \n",
    "\n",
    "- removes stopwords\n",
    "- removes punctuation signs\n",
    "- lemmatizes the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not (token.is_stop or token.is_punct)]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''These are the good times; Leave your cares behind'''\n",
    "print(preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a couple of punctuation signs with multi charaters as stopwords to the ```nlp``` spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add(\"`,\")\n",
    "nlp.Defaults.stop_words.add(\"``\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our preprocess function to the simplified Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df.text.apply(lambda txt : preprocess(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short text with just a few tokens won't have enough information for the classification model that we want to train. Let's add a count of the number of tokens for each text and filter out the text with less than N tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_count'] = df.processed_text.apply(lambda txt : len(txt.split())  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.token_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.token_count.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove texts that have less than 4 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.token_count > 4]\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.token_count > 4].topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df.processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is now a sparse matrix of 1258 rows by 4749 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the topic from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[df.topic == 'humor', 'topic' ] = 0\n",
    "df.loc[df.topic == 'science_fiction', 'topic' ] = 1\n",
    "df.topic = df.topic.astype(int)\n",
    "# define the target variable\n",
    "y = df.topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Declare the model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# 2. Train the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# 3. Make predictions \n",
    "yhat = clf.predict(X)\n",
    "\n",
    "# 4. score\n",
    "print(\"Accuracy: \",accuracy_score(y, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really train a classifier, we need to split the data in two parts: a training subset on which we train the model and a test set on which we evaluate the model. \n",
    "\n",
    "The test set simulates data that the model has not seen during its training and gives us a way to measure how the model extrapolates on unseen data.\n",
    "\n",
    "We need to split the data after we've vectorized the text, otherwise sont tokens may be present in the test set but not in the training set. \n",
    "\n",
    "To split the data into a train and a test set we use scikit's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the same model, this time on the train set and evaluate it on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Declare the model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# 2. Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions  on test set\n",
    "yhat = clf.predict(X_test)\n",
    "\n",
    "# 4. score\n",
    "print(\"Accuracy: \",accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is down to 77.4% which makes more sense than the outstanding 98% we previously obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "\n",
    "Let's now compare the model performance when we use a tf-idf vectorization approach instead of a simple tf / count vectorizer.\n",
    "\n",
    "The code is similar and we use scikit's default parameter for [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(df.processed_text)\n",
    "y = df.topic\n",
    "\n",
    "# split test train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# train and evaluate the model\n",
    "# 1. Declare the model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# 2. Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions  on test set\n",
    "yhat = clf.predict(X_test)\n",
    "\n",
    "# 4. score\n",
    "print(\"Accuracy: \",accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, not much difference between the 2 vectorizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# term to term matrix\n",
    "\n",
    "Here is the code to generate the term to term matrix in chapter 7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "# The corpus\n",
    "sentences = ['ways to replace the noisy Mac keyboard',\n",
    "'do you prefer Windows or Mac',\n",
    "'the Mac has a noisy keyboard',\n",
    "'ways to install Windows on a Mac',\n",
    "'you need a Windows license to install Windows on a Mac'\n",
    "]\n",
    "\n",
    "# vocab without stopwords\n",
    "vocab = sorted(list(set(word_tokenize(' '.join(sentences)))))\n",
    "stopwords = ['a', 'do', 'has', 'in', 'on', 'or', 'the', 'to', 'you', 'your']\n",
    "vocab = sorted([tk.lower() for tk in vocab if tk not in stopwords])\n",
    "\n",
    "print('Vocabulary:\\n',vocab,'\\n')\n",
    "\n",
    "# tokenize\n",
    "token_sent_list = [word_tokenize(sen) for sen in sentences]\n",
    "print('Each sentence in token form:\\n',token_sent_list,'\\n')\n",
    "\n",
    "# co occurrence window\n",
    "k=3\n",
    "\n",
    "# Definitely not an elegant way to create the term to term matrix\n",
    "co_occ = {ii:Counter({jj:0 for jj in vocab if jj!=ii}) for ii in vocab}\n",
    "\n",
    "\n",
    "for sen in token_sent_list:\n",
    "    sen = [tk.lower() for tk in sen if tk not in stopwords]\n",
    "    for ii in range(len(sen)):\n",
    "        if ii < k:\n",
    "            c = Counter(sen[0:ii+k+1])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "        elif ii > len(sen)-(k+1):\n",
    "            c = Counter(sen[ii-k::])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "        else:\n",
    "            c = Counter(sen[ii-k:ii+k+1])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "\n",
    "# Having final matrix in dict form lets you convert it to different python data structures\n",
    "co_occ = {ii:dict(co_occ[ii]) for ii in vocab}\n",
    "\n",
    "# convert to DataFrame\n",
    "\n",
    "df = pd.DataFrame(co_occ)\n",
    "df.fillna(0, inplace = True)\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].astype(int)\n",
    "\n",
    "# here goes\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
