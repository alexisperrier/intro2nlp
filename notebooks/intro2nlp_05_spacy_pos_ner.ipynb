{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7dc798",
   "metadata": {},
   "source": [
    "# POS and NER with Spacy\n",
    "\n",
    "\n",
    "[![Open In Colab](colab-badge.svg)](https://colab.research.google.com/github/alexisperrier/intro2nlp/blob/master/notebooks/intro2nlp_05_spacy_pos_ner.ipynb)\n",
    "\n",
    "\n",
    "POS tagging and NER are essential tasks in NLP. \n",
    "\n",
    "- POS is used for information extraction (finding all the adjectives associated with a person or a product, for example) and facilitates language understanding for complex NLP tasks (text generation, for instance). \n",
    "\n",
    "- NER is used across many domains to identify specific entities from the text (medical terms, legal concepts, people, â€¦). \n",
    "\n",
    "When parsing a text with a Spacy model: ```doc = nlp(text)```, Spacy also performs POS tagging and NER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy if you haven't done so already and download the small English model\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# install NLTK \n",
    "!pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy and the small English model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197260c",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging \n",
    "\n",
    "Let's start by exploring POS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3450f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"If you don't know where you are going any road can take you there.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# print the nature of each token\n",
    "for token in doc:\n",
    "   print(f\"{token.text}\\t {token.pos_} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now for some Shakespeare\n",
    "\n",
    "doc = nlp(\"Grace me no grace, nor uncle me no uncle\")\n",
    "for t in doc: \n",
    "    print(t, t.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07102fc",
   "metadata": {},
   "source": [
    "Spacy correctly identifies the nature of the _grace_ and _uncle_ both used as nouns (as expected) and as verbs.\n",
    "\n",
    "On the other hand, NLTK, is confused. Grace and Uncle are identified as nouns in all occurences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "text = nltk.word_tokenize(\"Grace me no grace, nor uncle me no uncle\")\n",
    "\n",
    "nltk.pos_tag(text,tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d0365",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Now let's see how we can extract names of peoples, places etc from a text with Spacy.\n",
    "\n",
    "And let's see which persons can be found in Alice in Wonderland\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# text from Alice in Wonderland\n",
    "r = requests.get('http://www.gutenberg.org/files/11/11-0.txt')\n",
    "\n",
    "# remove the footer and some weird characters \n",
    "# remove the header, the footer and some weird characters \n",
    "text = ' '.join(r.text.split('***')[1:])\n",
    "text = text.split(\"END OF THE PROJECT GUTENBERG\")[0]\n",
    "text = text.encode('ascii',errors='ignore').decode('utf-8')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba148b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and parse the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find all the 'persons' in the text\n",
    "persons = []\n",
    "# For each entity in the doc \n",
    "for ent in doc.ents:\n",
    "    # if the entity is a person\n",
    "    if ent.label_ == 'PERSON':\n",
    "        # add to the list of persons\n",
    "        persons.append(ent.text)\n",
    "\n",
    "# note we could have written the last bit in one line with\n",
    "persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "# list the 12 most common ones\n",
    "Counter(persons).most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3aabe",
   "metadata": {},
   "source": [
    "The Rabbit, although a very frequent character in the book, doesn't come out in the top 20 of identified persons. \n",
    "\n",
    "Let's see how the Rabbit entity is classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b82d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rabbit_ner = [(ent.text, ent.label_) for ent in doc.ents if \"Rabbit\" in ent.text]\n",
    "Counter(rabbit_ner).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9414e25",
   "metadata": {},
   "source": [
    "Interestingly, the Rabbit is identified as a location, an event and even a work of art! But not as a person.\n",
    "\n",
    "Let's see if we get better results by using a larger Spacy model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a195d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the large English model.\n",
    "# Note: Better to comment out the line after you've downladed the model the first time \n",
    "# to avoid downloading it each time you run the notebook!\n",
    "!python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e65a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_lg = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and parse the text this time with the large language model\n",
    "\n",
    "# and parse the text\n",
    "doc = nlp_lg(text)\n",
    "\n",
    "# Find all the 'persons' in the text\n",
    "persons = []\n",
    "# For each entity in the doc \n",
    "for ent in doc.ents:\n",
    "    # if the entity is a person\n",
    "    if ent.label_ == 'PERSON':\n",
    "        # add to the list of persons\n",
    "        persons.append(ent.text)\n",
    "\n",
    "# note we could have written the last bit in one line with\n",
    "persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "# list the 12 most common ones\n",
    "Counter(persons).most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "rabbit_ner = [(ent.text, ent.label_) for ent in doc.ents if \"Rabbit\" in ent.text]\n",
    "Counter(rabbit_ner).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9bc6b5",
   "metadata": {},
   "source": [
    "Well that did not really work out either. The poor rabbit is now an organisation and still not a person or character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93065c2c",
   "metadata": {},
   "source": [
    "Note that with the larger model, Alice is identified as a Person 293 but with the smaller model, Alice is a person only 191 times. So although, the model still can't identify the entity class of the Rabbit, it does a better job on other characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d3404",
   "metadata": {},
   "source": [
    "Let's see which other ORGs we can find in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55930bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
    "Counter(orgs).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37eb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and work of art\n",
    "\n",
    "woas = [ent.text for ent in doc.ents if ent.label_ == 'WORK_OF_ART']\n",
    "Counter(woas).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378172f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
